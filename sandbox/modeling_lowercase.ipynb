{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import boto3\n",
    "# from collections import defaultdict\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Raghu's code to pull down jobs\n",
    "BUCKET_NAME = 'tech-salary-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# bucket = s3.Bucket(BUCKET_NAME)\n",
    "# all_job_titles = []\n",
    "# all_job_ids = []\n",
    "# all_summaries = []\n",
    "# for o in bucket.objects.all():\n",
    "#     if o.key.startswith('salaries'):\n",
    "#         continue\n",
    "    \n",
    "#     object = bucket.Object(o.key)\n",
    "#     try:\n",
    "#         lines = object.get()['Body'].read().decode('utf-8').splitlines()\n",
    "#         for line in lines:\n",
    "#             d = json.loads(line)\n",
    "            \n",
    "#             title = d['title']\n",
    "#             jid = d['id']\n",
    "#             summary = d['summary']\n",
    "            \n",
    "#             all_job_ids.append(jid)\n",
    "#             all_job_titles.append(title)\n",
    "#             all_summaries.append(summary)\n",
    "#     except Exception as e:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119326\n"
     ]
    }
   ],
   "source": [
    "print(len(set(all_summaries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 13.2 s, total: 30.6 s\n",
      "Wall time: 26min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Put dataset in memory\n",
    "directory = os.fsencode('../local_data/')\n",
    "\n",
    "original_texts = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    full_file = '../local_data/' + str(filename)\n",
    "    with open(full_file, 'r') as infile:\n",
    "        text = infile.read()\n",
    "        original_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4290016\n",
      "519155\n",
      "CPU times: user 11.7 s, sys: 325 ms, total: 12 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load in Stackoverflow data\n",
    "\n",
    "stackoverflow_raw = []\n",
    "\n",
    "def clean_text(text):\n",
    "    cleanr = re.compile(r'\\bamp\\b')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "with open('../stackoverflow_data/stack_parsed.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        line = clean_text(line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        stackoverflow_raw.append(line)\n",
    "\n",
    "# Size of list in bytes\n",
    "print(sys.getsizeof(stackoverflow_raw))\n",
    "print(len(stackoverflow_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of job data list: 106373\n",
      "Length of Stackoverflow data list: 519155\n",
      "Combined dataset length: 625528\n",
      "Size of total dataset in MB: 5.004288\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of job data list:\", len(original_texts))\n",
    "print(\"Length of Stackoverflow data list:\", len(stackoverflow_raw))\n",
    "all_data = original_texts + stackoverflow_raw\n",
    "print(\"Combined dataset length:\", len(all_data))\n",
    "\n",
    "print(\"Size of total dataset in MB:\", sys.getsizeof(all_data)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 236 ms, total: 640 ms\n",
      "Wall time: 638 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lower_stackoverflow = [item.lower().strip() for item in stackoverflow_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate approach, 16APR18: Manually replace bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in subset: 63\n"
     ]
    }
   ],
   "source": [
    "# subset = lower_data[::10000]\n",
    "# print(\"Items in subset:\",len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning becomes machine_learning\n"
     ]
    }
   ],
   "source": [
    "# Load pickled skill dictionary from disk. I generated it from prior work.\n",
    "with open('skill_bigram_dict.pkl', 'rb') as handle:\n",
    "    skill_dict = pickle.load(handle)\n",
    "print(\"machine learning becomes\", skill_dict['machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text):\n",
    "    '''\n",
    "    Function that uses regex to replace terms.\n",
    "    Input is a dictionary of terms to switch, plus\n",
    "    a string.\n",
    "    '''\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 56s, sys: 159 ms, total: 4min 56s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# new_data = []\n",
    "# for text in lower_data:\n",
    "#     new_data.append(multiple_replace(skill_dict, text))\n",
    "\n",
    "new_stackoverflow = []\n",
    "for text in lower_stackoverflow:\n",
    "    new_stackoverflow.append(multiple_replace(skill_dict, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write list to file as a pickled object, to save time later\n",
    "with open('../models_12apr/stackoverflow_data_skill_bigrams_list.pkl', 'wb') as f:\n",
    "    pickle.dump(new_stackoverflow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load back into memory\n",
    "with open('../models_12apr/all_lowercase_data_skill_bigrams_list.pkl', 'rb') as handle:\n",
    "    new_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for index, item in enumerate(new_data):\n",
    "#     if 'getelementbyid' in item:\n",
    "#         print(\"Found!\", index, item)\n",
    "\n",
    "# print(lemmatized_sentence_corpus(new_data[106374].ipynb_checkpoints/ipynb_checkpoints/\n",
    "                                 \n",
    "# for sentence in lemmatized_sentence_corpus(new_data[106374:106375]):\n",
    "#     print(sentence + '\\n')\n",
    "\n",
    "with open('../models_12apr/jds_skill_bigrams_concat_parsed.txt', 'r') as infile:\n",
    "    for index,line in enumerate(infile[4000000:]):\n",
    "        if 'while apply opacity to a form should -PRON- use a decimal or a double value' in line:\n",
    "            print('Found!')\n",
    "            print(index, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: All texts are lowercase at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    12APR: adding numbers to this\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    SRG: modified for a list\n",
    "    generator function to read in text from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    for text in filename:\n",
    "        yield text.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_text in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=5000, n_threads=24):\n",
    "        \n",
    "        for sent in parsed_text.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1d 1h 16min 47s, sys: 1d 13h 27min 12s, total: 2d 14h 44min\n",
      "Wall time: 3h 40min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open('../models_12apr/jds_skill_bigrams_concat_parsed_stackoverflow.txt', 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(new_stackoverflow):\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Took 5 hrs 37 minutes last time.\n",
    "# Started: 3:30 PM; ETA is 9 PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence('../models_05apr/jds_concat_parsed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522690 disney\n",
      "\n",
      "30 + day ago\n",
      "\n",
      "the project manager be experience in the use of project management tool and principle who can manage activity for cross functional team within technical operations\n",
      "\n",
      "the project manager will ensure the project be state of the art plan and document stay on track and be within the expected budget and timeline\n",
      "\n",
      "the project manager work closely with the strategic program lead to guide the team in develop project and program plan which bring the overall program successfully through the lifecycle process and be in alignment with product strategy management and health authorities’ requirement\n",
      "\n",
      "this role will include 50 team management and 50 project management include oversee project activity and ensure alignment with global strategy identify risk and develop mitigation plan establish team goal milestone update progress against milestone and communicate with stakeholder and senior management regard progress of project\n",
      "\n",
      "responsibilities\n",
      "\n",
      "support\n",
      "\n",
      "right\n",
      "\n",
      "first time lead and sub team coordinates and schedule team meeting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some examples\n",
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 17s, sys: 4.29 s, total: 4min 22s\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save('../models_05apr/bigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load('../models_05apr/bigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 9s, sys: 1.12 s, total: 9min 10s\n",
      "Wall time: 9min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open('../models_05apr/bigram_sentences.txt', 'w', encoding='utf_8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence('../models_05apr/bigram_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print examples; certain bigrams are underlined\n",
    "# for bigram_sentence in it.islice(bigram_sentences, 240, 250):\n",
    "#     print(u' '.join(bigram_sentence))\n",
    "#     print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 45s, sys: 4.68 s, total: 4min 50s\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_model = Phrases(bigram_sentences)\n",
    "trigram_model.save('../models_05apr/trigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load('../models_05apr/trigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodgame/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 55s, sys: 3.09 s, total: 9min 59s\n",
      "Wall time: 9min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open('../models_05apr/trigram_sentences.txt', 'w', encoding='utf_8') as f:\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence('../models_05apr/trigram_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "establishes and maintain information sharing and communication structure e.g. sharepoint with current version of meeting_agenda_minute and presentation_material and deliverable draft and final\n",
      "\n",
      "support\n",
      "\n",
      "the right first time lead in the execution of program by translation of define strategy into team goal and milestone in alignment with regulation\n",
      "\n",
      "owns program documentation include program plan and schedule meet agenda and minute action item dashboard goal and milestone risk register mitigation_plan decision log etc\n",
      "\n",
      "resolving project constraint and prioritization of task in alignment with the program\n",
      "\n",
      "facilitate the use of project management tool to achieve a balance datum drive and risk base decision_making process definition of mitigation\n",
      "\n",
      "and/or contingency_plan executes\n",
      "\n",
      "scenario planning and risk_assessment develop option for the team ensures project progress and upcoming deliverable be understand by the team by use pm tool\n",
      "\n",
      "holds team_member accountable for on time completion of task or milestone assign to support project plan\n",
      "\n",
      "creates and update project plan reports project status into governance meeting responsible for project budget_resource_allocation tracking and oversee capital_expenditure as appropriate acts_as resource for development implementation of project management tool and methodology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 240, 250):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \"'s\", 'or']\n"
     ]
    }
   ],
   "source": [
    "# NLTK stopwords format is not a list; use this version\n",
    "stopwords = get_stop_words('english')\n",
    "additional_stopwords = [\"\\'s\", 'or']\n",
    "stopwords = stopwords + additional_stopwords\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodgame/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1d 22h 31min 16s, sys: 2d 18h 36min 50s, total: 4d 17h 8min 6s\n",
      "Wall time: 7h 53min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open('../models_05apr/final_sentences.txt', 'w', encoding='utf_8') as f:\n",
    "    for parsed_review in nlp.pipe(line_review(all_data),\n",
    "                                  batch_size=5000, n_threads=24):\n",
    "        # lemmatize the text, removing punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                          if not punct_space(token)]\n",
    "\n",
    "        # apply the first-order and second-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term not in stopwords]\n",
    "\n",
    "        # write the transformed review as a line in the new file\n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625528 ../models_05apr/final_sentences.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../models_05apr/final_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probably another memory issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 3.94 s, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trigram_reviews = LineSentence('../models_05apr/final_sentences.txt')\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save('../models_05apr/trigram_dictionary.dict')\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load('../models_05apr/trigram_dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 0 ns, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate bag-of-words representations for all JDs and save them as a matrix\n",
    "MmCorpus.serialize('../models_05apr/trigram_corpus.mm',\n",
    "                   trigram_bow_generator('../models_05apr/final_sentences.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
