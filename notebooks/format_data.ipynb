{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data\n",
    "\n",
    "## Preprocess raw text prior to modeling\n",
    "\n",
    "Note: my initial approach involved using spaCy bigram and trigram modeling. I abandoned that approach because it was yielding terms like `java_python` or `docker_kubernetes` -- i.e., it was connecting skills that commonly appeared together.\n",
    "\n",
    "That code exists in the notebook `modeling_lowercase.ipynb` in the `/sandbox` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/goodgame/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import boto3\n",
    "# from collections import defaultdict\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'tech-salary-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# bucket = s3.Bucket(BUCKET_NAME)\n",
    "# all_job_titles = []\n",
    "# all_job_ids = []\n",
    "# all_summaries = []\n",
    "# for o in bucket.objects.all():\n",
    "#     if o.key.startswith('salaries'):\n",
    "#         continue\n",
    "    \n",
    "#     object = bucket.Object(o.key)\n",
    "#     try:\n",
    "#         lines = object.get()['Body'].read().decode('utf-8').splitlines()\n",
    "#         for line in lines:\n",
    "#             d = json.loads(line)\n",
    "            \n",
    "#             title = d['title']\n",
    "#             jid = d['id']\n",
    "#             summary = d['summary']\n",
    "            \n",
    "#             all_job_ids.append(jid)\n",
    "#             all_job_titles.append(title)\n",
    "#             all_summaries.append(summary)\n",
    "#     except Exception as e:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119326\n"
     ]
    }
   ],
   "source": [
    "print(len(set(all_summaries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 13.2 s, total: 30.6 s\n",
      "Wall time: 26min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Put dataset in memory\n",
    "directory = os.fsencode('../local_data/')\n",
    "\n",
    "original_texts = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    full_file = '../local_data/' + str(filename)\n",
    "    with open(full_file, 'r') as infile:\n",
    "        text = infile.read()\n",
    "        original_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4290016\n",
      "519155\n",
      "CPU times: user 11.7 s, sys: 325 ms, total: 12 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load in Stackoverflow data\n",
    "\n",
    "stackoverflow_raw = []\n",
    "\n",
    "def clean_text(text):\n",
    "    cleanr = re.compile(r'\\bamp\\b')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "with open('../stackoverflow_data/stack_parsed.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        line = clean_text(line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        stackoverflow_raw.append(line)\n",
    "\n",
    "# Size of list in bytes\n",
    "print(sys.getsizeof(stackoverflow_raw))\n",
    "print(len(stackoverflow_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of job data list: 106373\n",
      "Length of Stackoverflow data list: 519155\n",
      "Combined dataset length: 625528\n",
      "Size of total dataset in MB: 5.004288\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of job data list:\", len(original_texts))\n",
    "print(\"Length of Stackoverflow data list:\", len(stackoverflow_raw))\n",
    "all_data = original_texts + stackoverflow_raw\n",
    "print(\"Combined dataset length:\", len(all_data))\n",
    "\n",
    "print(\"Size of total dataset in MB:\", sys.getsizeof(all_data)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 236 ms, total: 640 ms\n",
      "Wall time: 638 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lower_stackoverflow = [item.lower().strip() for item in stackoverflow_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually replace bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in subset: 63\n"
     ]
    }
   ],
   "source": [
    "# subset = lower_data[::10000]\n",
    "# print(\"Items in subset:\",len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning becomes machine_learning\n"
     ]
    }
   ],
   "source": [
    "# Load pickled skill dictionary from disk. I generated it from prior work.\n",
    "with open('skill_bigram_dict.pkl', 'rb') as handle:\n",
    "    skill_dict = pickle.load(handle)\n",
    "print(\"machine learning becomes\", skill_dict['machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text):\n",
    "    '''\n",
    "    Function that uses regex to replace terms.\n",
    "    Input is a dictionary of terms to switch, plus\n",
    "    a string.\n",
    "    '''\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 56s, sys: 159 ms, total: 4min 56s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# new_data = []\n",
    "# for text in lower_data:\n",
    "#     new_data.append(multiple_replace(skill_dict, text))\n",
    "\n",
    "new_stackoverflow = []\n",
    "for text in lower_stackoverflow:\n",
    "    new_stackoverflow.append(multiple_replace(skill_dict, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write list to file as a pickled object, to save time later\n",
    "with open('../models_12apr/stackoverflow_data_skill_bigrams_list.pkl', 'wb') as f:\n",
    "    pickle.dump(new_stackoverflow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load back into memory\n",
    "with open('../models_12apr/all_lowercase_data_skill_bigrams_list.pkl', 'rb') as handle:\n",
    "    new_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: All texts are lowercase at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    12APR: adding numbers to this\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    SRG: modified for a list\n",
    "    generator function to read in text from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    for text in filename:\n",
    "        yield text.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_text in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=5000, n_threads=24):\n",
    "        \n",
    "        for sent in parsed_text.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1d 1h 16min 47s, sys: 1d 13h 27min 12s, total: 2d 14h 44min\n",
      "Wall time: 3h 40min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open('../models_12apr/jds_skill_bigrams_concat_parsed_stackoverflow.txt', 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(new_stackoverflow):\n",
    "        f.write(sentence + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
