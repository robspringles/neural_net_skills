{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook includes code to evaluate models and conduct error analysis.\n",
    "\n",
    "If some of the objects don't load properly, it's because they have been placed into a more organized directory structure (i.e., the raw data exists in `/data`, the pickled error dictionaries exist in `/evaluation/errors/dicts`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import pickle\n",
    "import wikipedia as wik\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from collections import defaultdict\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "# Black magic\n",
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.attrs import *\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentences from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "wikipedia_skills = []\n",
    "with open('clear_terms.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        wikipedia_skills.append(line)\n",
    "print(len(wikipedia_skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 1.22 s, total: 16.1 s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ambiguous_terms = []\n",
    "clear_terms = {}\n",
    "page_error_terms = []\n",
    "\n",
    "for term in wikipedia_skills:\n",
    "    try:\n",
    "        text = wik.summary(term)\n",
    "        sents = text.split('. ')\n",
    "        summary = '. '.join(sents)\n",
    "        clear_terms[term] = summary\n",
    "    except wik.exceptions.DisambiguationError as e:\n",
    "        ambiguous_terms.append(term)\n",
    "        continue\n",
    "    except wik.exceptions.PageError as e:\n",
    "        page_error_terms.append(term)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous terms: 3\n",
      "\t ai\n",
      "\t rdf\n",
      "\t wix\n",
      "Page errors 1\n",
      "\t pypi\n"
     ]
    }
   ],
   "source": [
    "# print(clear_terms[\"machine learning\"][:150])\n",
    "print(\"Ambiguous terms:\", len(ambiguous_terms))\n",
    "for term in ambiguous_terms:\n",
    "    print(\"\\t\", term)\n",
    "print(\"Page errors\", len(page_error_terms))\n",
    "for term in page_error_terms:\n",
    "    print(\"\\t\", term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n"
     ]
    }
   ],
   "source": [
    "print(len(clear_terms.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clear_terms_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(clear_terms, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open('clear_terms_dict.pkl', 'rb') as f:\n",
    "    clear_terms = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** .net framework \t ******************************\n",
      ".NET Framework (pronounced dot net) is a software framework developed by Microsoft that runs primarily on Microsoft Windows. It includes a large class library named Framework Class Library (FCL) and provides language interoperability (each language can use code written in other languages) across several programming languages. Programs written for .NET Framework execute in a software environment (in contrast to a hardware environment) named Common Language Runtime (CLR), an application virtual machine that provides services such as security, memory management, and exception handling. (As such, computer code written using .NET Framework is called \"managed code\".) FCL and CLR together constitute .NET Framework.\n",
      "FCL provides user interface, data access, database connectivity, cryptography, web application development, numeric algorithms, and network communications. Programmers produce software by combining their source code with .NET Framework and other libraries. The framework is intended to be used by most new applications created for the Windows platform. Microsoft also produces an integrated development environment largely for .NET software called Visual Studio.\n",
      ".NET Framework began as proprietary software, although the firm worked to standardize the software stack almost immediately, even before its first release. Despite the standardization efforts, developers, mainly those in the free and open-source software communities, expressed their unease with the selected terms and the prospects of any free and open-source implementation, especially regarding software patents. Since then, Microsoft has changed .NET development to more closely follow a contemporary model of a community-developed software project, including issuing an update to its patent promising to address the concerns.\n",
      ".NET Framework led to a family of .NET platforms targeting mobile computing, embedded devices, alternative operating systems, and web browser plug-ins. A reduced version of the framework, .NET Compact Framework, is available on Windows CE platforms, including Windows Mobile devices such as smartphones. .NET Micro Framework is targeted at very resource-constrained embedded devices. Silverlight was available as a web browser plugin. Mono is available for many operating systems and is customized into popular smartphone operating systems (Android and iOS) and game engines. .NET Core targets the Universal Windows Platform (UWP), and cross-platform and cloud computing workloads. \n",
      "\n",
      "\n",
      "********** a/b testing \t ******************************\n",
      "In web analytics, A/B testing (bucket tests or split-run testing) is a controlled experiment with two variants, A and B.  It is a form of statistical hypothesis testing or \"two-sample hypothesis testing\" as used in the field of statistics. A/B testing is a way to compare two versions of a single variable typically by testing a subject's response to variable A against variable B, and determining which of the two variables is more effective.\n",
      "As the name implies, two versions (A and B) are compared, which are identical except for one variation that might affect a user's behavior. Version A might be the currently used version (control), while version B is modified in some respect (treatment). For instance, on an e-commerce website the purchase funnel is typically a good candidate for A/B testing, as even marginal improvements in drop-off rates can represent a significant gain in sales. Significant improvements can sometimes be seen through testing elements like copy text, layouts, images and colors, but not always.\n",
      "Multivariate testing or multinomial testing is similar to A/B testing, but may test more than two versions at the same time or use more controls. Simple A/B tests are not valid for observational, quasi-experimental or other non-experimental situations, as is common with survey data, offline data, and other, more complex phenomena.\n",
      "A/B testing has been marketed by some as a change in philosophy and business strategy in certain niches, though the approach is identical to a between-subjects design, which is commonly used in a variety of research traditions. A/B testing as a philosophy of web development brings the field into line with a broader movement toward evidence-based practice. The benefits of A/B testing are considered to be that it can be performed continuously on almost anything, especially since most marketing automation software now, typically, comes with the ability to run A/B tests on an ongoing basis. This allows for updating websites and other tools, using current resources, to keep up with changing trends.\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "********** actionscript \t ******************************\n",
      "ActionScript is an object-oriented programming language originally developed by Macromedia Inc. (later acquired by Adobe Systems). It is a derivation of HyperTalk, the scripting language for HyperCard. It is now a dialect of ECMAScript (meaning it is a superset of the syntax and semantics of the language more widely known as JavaScript), though it originally arose as a sibling, both being influenced by HyperTalk.\n",
      "ActionScript is used primarily for the development of websites and software targeting the Adobe Flash Player platform, used on Web pages in the form of embedded SWF files.\n",
      "ActionScript 3 is also used with Adobe AIR system for the development of desktop and mobile applications. The language itself is open-source in that its specification is offered free of charge and both an open source compiler (as part of Apache Flex) and open source virtual machine (Mozilla Tamarin) are available.\n",
      "ActionScript is also used with Scaleform GFx for the development of 3D video game user interfaces and HUDs. \n",
      "\n",
      "\n",
      "********** activerecord \t ******************************\n",
      "In software engineering, the active record pattern is an architectural pattern found in software that stores in-memory object data in relational databases. It was named by Martin Fowler in his 2003 book Patterns of Enterprise Application Architecture. The interface of an object conforming to this pattern would include functions such as Insert, Update, and Delete, plus properties that correspond more or less directly to the columns in the underlying database table.\n",
      "The active record pattern is an approach to accessing data in a database. A database table or view is wrapped into a class. Thus, an object instance is tied to a single row in the table. After creation of an object, a new row is added to the table upon save. Any object loaded gets its information from the database. When an object is updated, the corresponding row in the table is also updated. The wrapper class implements accessor methods or properties for each column in the table or view.\n",
      "This pattern is commonly used by object persistence tools and in object-relational mapping (ORM). Typically, foreign key relationships will be exposed as an object instance of the appropriate type via a property. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore some sentences\n",
    "ctr = 0\n",
    "for key, value in clear_terms.items():\n",
    "    ctr += 1\n",
    "    if ctr < 5:\n",
    "        print('*'*10, key, '\\t', '*'*30)\n",
    "        print(value, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace skill bigrams with properly underscored skill bigrams\n",
    "\n",
    "This is necessary for the model to correctly understand terms like \"machine learning.\" For bigram skills, I replace spaces separating words with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pickle\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "    text = text.lower()\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \n",
    "\n",
    "# print(multiple_replace(dict, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = {}\n",
    "with open('skills_original.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        underscores = line.replace(' ', '_')\n",
    "        bigram_dict[line] = underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i like to do lots of machine_learning and data_science.\n",
      "i enjoy training neural_networks and neural_nets.\n",
      "i fill my code repository with sandbags.\n",
      "i like creative cloud, creative suite, destructive_testing, and go_lang.\n"
     ]
    }
   ],
   "source": [
    "input_text = '''\n",
    "I like to do lots of Machine learning and data science.\n",
    "I enjoy training neural networks and neural nets.\n",
    "I fill my code repository with sandbags.\n",
    "I like creative cloud, creative suite, destructive testing, and go lang.'''\n",
    "\n",
    "print(multiple_replace(bigram_dict, input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences that contain the skills in which we're interested\n",
    "import pickle\n",
    "import random\n",
    "# Load up skills dictionary\n",
    "with open('skill_dict.pkl', 'rb') as f:\n",
    "    skill_dict = pickle.load(f)\n",
    "    \n",
    "skills_list = []\n",
    "# Load list of skills to match\n",
    "with open('skills_original.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if line == '.net':\n",
    "            continue # added 20APR to avoid confusion with .net items\n",
    "        if line == 'vb.net':\n",
    "            continue\n",
    "        if line == 'ado.net':\n",
    "            continue\n",
    "        skills_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915\n"
     ]
    }
   ],
   "source": [
    "print(len(skills_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences that contain hard skills: 2182\n",
      "\n",
      "\n",
      "Samples:\n",
      " ['bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression', 'bagging is a special case of the model averaging approach.', 'balsamiq studios is an isv founded in march 2008 by peldi guilizzoni, a former adobe senior software engineer', 'the web-based balsamiq mockup tool was launched in june 2008']\n",
      "CPU times: user 3.36 s, sys: 4.69 ms, total: 3.37 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_sentences = []\n",
    "for chunk in clear_terms.values():\n",
    "    chunk = chunk.replace('.\\n', '. ').lower().strip()\n",
    "    sentences = chunk.split('. ')\n",
    "    for s in sentences:\n",
    "        for skill in skills_list:\n",
    "            if skill in s.split():\n",
    "                correct_sentences.append(s.replace('asp.net','')\\\n",
    "                                         .replace(' ado.net','').replace('asp.net','')\\\n",
    "                                        .replace('.net','')) #20APR18\n",
    "                break\n",
    "print(\"Number of sentences that contain hard skills:\", len(correct_sentences))\n",
    "print(\"\\n\\nSamples:\\n\",correct_sentences[145:149])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sentences = list(set(correct_sentences))\n",
    "len(correct_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format sentences and return proper bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i and -PRON- be machine_learn and neural $ network user with 501(c)3 .net skill\n",
      "i and -PRON- be machine learning and neural $ network user with 501(c)3 .net skill\n"
     ]
    }
   ],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    12APR: adding numbers to this\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def format_text(text, bigram_dict):\n",
    "    text = multiple_replace(bigram_dict, text)\n",
    "    doc = nlp(text) \n",
    "    return u' '.join([token.lemma_ for token in doc\n",
    "                         if not punct_space(token)])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text) \n",
    "    return u' '.join([token.lemma_ for token in doc\n",
    "                         if not punct_space(token)])\n",
    "\n",
    "# Necessary step to make the sentences exactly the same\n",
    "def tokenize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    bigram_input_list = tokenizer.tokenize(text)\n",
    "    return ' '.join(bigram_input_list)\n",
    "\n",
    "sent = 'i and you are machine learning and neural $ network users with 501(c)3 .net skills.'\n",
    "\n",
    "print(format_text(sent, bigram_dict))\n",
    "print(lemmatize_text(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: as of android studio 3.0 october kotlin be a fully support programming_language by google on the android operating_system and be directly include in the android studio 3.0 ide package as an alternative to the standard java compiler\n",
      "Correct as of android studio 3 0 october kotlin be a fully support programming_language by google on the android operating_system and be directly include in the android studio 3 0 ide package as an alternative to the standard java compiler\n"
     ]
    }
   ],
   "source": [
    "sample = '''Correct: as of android studio 3.0 october kotlin be a fully support programming_language by google on the android operating_system and be directly include in the android studio 3.0 ide package as an alternative to the standard java compiler'''\n",
    "\n",
    "print(sample)\n",
    "print(tokenize_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS: as of android studio 3 0 20 october kubernetes be a fully support programming_language by google on the android operating_system and be directly include in the android studio 3 0 ide package as an alternative to the standard elastic compiler\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "def corrupt_hard_skills(text):\n",
    "    # Construct matcher object\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    doc = nlp(text) \n",
    "    for label, pattern in skill_dict.items():\n",
    "        matcher.add(label, None, pattern)\n",
    "    # Compare input to pre-defined skill patterns\n",
    "    user_skills = []\n",
    "    matches = matcher(doc) \n",
    "    for match in matches:\n",
    "        if match is not None:\n",
    "            # match object returns a tuple with (id, startpos, endpos)\n",
    "            output = str(doc[match[1]:match[2]]).lower()\n",
    "            user_skills.append(output)\n",
    "    \n",
    "    num_hard_skills = len(user_skills)\n",
    "    random_hard_skills = []\n",
    "    for num in range(len(user_skills)):\n",
    "        random_hard_skills.append(skills_list[random.randrange(len(skills_list))])\n",
    "        \n",
    "    # Make sure everything that needs to be a bigram is a bigram\n",
    "    bigram_input = multiple_replace(bigram_dict, text)\n",
    "    user_skills = [multiple_replace(bigram_dict, item) for item in user_skills]\n",
    "    random_hard_skills = [multiple_replace(bigram_dict, item) for item in random_hard_skills]\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    bigram_input_list = tokenizer.tokenize(bigram_input)\n",
    "    \n",
    "    output = []\n",
    "    for word in bigram_input_list:\n",
    "        if word not in user_skills:\n",
    "            output.append(word)\n",
    "        else:\n",
    "            for index, skill in enumerate(user_skills):\n",
    "                if word == skill:\n",
    "                    output.append(random_hard_skills[index])\n",
    "    return ' '.join(output)\n",
    "\n",
    "\n",
    "input_str = 'i like to do machine learning, c, r, .net and neural network.'\n",
    "sample = '''as of android studio 3.0 $20 october kotlin be a fully support programming_language by google on the android operating_system and be directly include in the android studio 3.0 ide package as an alternative to the standard java compiler\n",
    "'''\n",
    "\n",
    "# print(format_text(corrupt_hard_skills(input_str), bigram_dict))\n",
    "print(\"\\nRESULTS:\",corrupt_hard_skills(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as of android studio october kotlin be a fully support programming_language by google on the android operating_system and be directly include in the android studio ide package as an alternative to the standard java compiler\n",
      "\n",
      "\n",
      "as of android studio october statistical_package be a fully support programming_language by google on the android operating_system and be directly include in the android studio ide package as an alternative to the standard hypervisor compiler\n"
     ]
    }
   ],
   "source": [
    "## Proper processing for correct and incorrect strings\n",
    "print(format_text(tokenize_text(sample), bigram_dict))\n",
    "print('\\n')\n",
    "print(format_text(corrupt_hard_skills(sample), bigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 1.6 s, total: 31.6 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_sentences_formatted = [format_text(tokenize_text(sentence), bigram_dict) for sentence in correct_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 3.22 s, total: 1min 21s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "incorrect_sentences_formatted = []\n",
    "for sentence in correct_sentences:\n",
    "    incorrect_sentences_formatted.append(format_text(corrupt_hard_skills(sentence), bigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2114\n",
      "2114\n"
     ]
    }
   ],
   "source": [
    "print(len(incorrect_sentences_formatted))\n",
    "print(len(correct_sentences_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(correct_sentences_formatted):\n",
    "    if '.net' in item:\n",
    "        print(item)\n",
    "        print(incorrect_sentences_formatted[index])\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(\"Clean:\", correct_sentences_formatted[i])\n",
    "    print(\"Corrupted:\", incorrect_sentences_formatted[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write correct and incorrect sentences to disk, to save time later\n",
    "with open('correct_2114_sentences_list.pkl', 'wb') as f:\n",
    "    pickle.dump(correct_sentences_formatted, f)\n",
    "with open('incorrect_2114_sentences_list.pkl', 'wb') as f:\n",
    "    pickle.dump(incorrect_sentences_formatted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them back up\n",
    "# Write correct and incorrect sentences to disk, to save time later\n",
    "with open('correct_2114_sentences_list.pkl', 'rb') as f:\n",
    "    correct_sentences_formatted = pickle.load(f)\n",
    "with open('incorrect_2114_sentences_list.pkl', 'rb') as f:\n",
    "    incorrect_sentences_formatted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test probabilities of sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 2 training epochs.\n"
     ]
    }
   ],
   "source": [
    "word2vec_filepath = 'models/word2vec_hs1_neg0'\n",
    "# load the finished model from disk\n",
    "skill2vec = Word2Vec.load(word2vec_filepath)\n",
    "skill2vec.init_sims()\n",
    "print(u'Model loaded with {} training epochs.'.format(skill2vec.train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(skill2vec_model, correct_sentence_list, incorrect_sentence_list, num_total_sentences=8633141):\n",
    "    '''\n",
    "    Ingest a list of (properly formatted) correct sentences and incorrect sentences.\n",
    "    For each list, score the negative log likelihood of each sentence.\n",
    "    Compare the averages, and output the correct average minus the incorrect average.\n",
    "    Higher results indicate a better model.\n",
    "    '''\n",
    "    correct_score = 0.0\n",
    "    for sentence in correct_sentence_list:\n",
    "        correct_score += -1*(skill2vec_model.score([sentence.split()], total_sentences=num_total_sentences))\n",
    "    correct_avg = correct_score/len(correct_sentence_list)\n",
    "    \n",
    "    incorrect_score = 0.0\n",
    "    for sentence in incorrect_sentence_list:\n",
    "        incorrect_score += -1*(skill2vec_model.score([sentence.split()], total_sentences=num_total_sentences))\n",
    "    incorrect_avg = incorrect_score/len(incorrect_sentence_list)\n",
    "    return incorrect_avg[0] - correct_avg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-329.62134"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error analysis\n",
    "sample_good = ['kubernete commonly stylize as k8s be an open source system for automate deployment scaling and management of containerized application that be originally design by google and now maintain by the cloud native computing foundation']\n",
    "sample_bad = ['google_compute_engine stylize as k8s be an open source system for automate deployment scaling and management of ramdajs application that be originally design by google and now maintain by the cloud native computing foundation']\n",
    "test_models(skill2vec, sample_good, sample_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(test_models(skill2vec, correct_sentences_formatted, incorrect_sentences_formatted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into full text models and JD-only models. \n",
    "\n",
    "directory = 'models/'\n",
    "\n",
    "full_text_models = ['word2vec_hs1_neg0',\n",
    "                    'word2vec_hs1_neg0_cbow',\n",
    "                    'word2vec_hs1_neg0_size50_cbow',\n",
    "                    'word2vec_hs1_neg0_size200',\n",
    "                    'word2vec_hs1_neg0_window_8',\n",
    "                    'word2vec_size_400_hs1_neg0',\n",
    "                    'word2vec_size_600_hs1_neg0', \n",
    "                    'word2vec_hs1_neg0_window_10_size300',\n",
    "                    'word2vec_hs1_neg0_window_12_size600',\n",
    "                   'word2vec_hs1_neg0_window_15_size300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 11s, sys: 3min 4s, total: 7min 16s\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "# Full text models have 8633141 (the default) sentences\n",
    "for model in full_text_models:\n",
    "    path = directory + model\n",
    "    skill2vec_model = Word2Vec.load(path)\n",
    "    model_performance[model] = test_models(skill2vec_model, \n",
    "                             correct_sentences_formatted, \n",
    "                             incorrect_sentences_formatted)\n",
    "\n",
    "# Print out sorted model values\n",
    "\n",
    "with open('errors/performance.txt', 'w') as outfile:\n",
    "    s = [(k, model_performance[k]) for k in sorted(model_performance, \n",
    "                                                       key=model_performance.get, reverse=True)]\n",
    "\n",
    "    for k, v in s:\n",
    "        out = str(k) + '\\t' + str(v) + '\\n'\n",
    "        outfile.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep track of the skills associated with failing comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.57 s, sys: 397 ms, total: 6.96 s\n",
      "Wall time: 5.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "post_processing_skills = []\n",
    "\n",
    "for val in bigram_dict.values():\n",
    "    processed = lemmatize_text(val)\n",
    "    post_processing_skills.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_underscores = [item.replace(' ', '_') for item in skills_list]\n",
    "exhaustive_skills = skill_underscores + post_processing_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hard_skills(text):\n",
    "    '''\n",
    "    Input a text, output a list of hard skills.\n",
    "    '''\n",
    "    skills = []\n",
    "    for word in text.split():\n",
    "        if word in exhaustive_skills:\n",
    "            skills.append(word)\n",
    "    return(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentence_fails(skill2vec_model, correct_sentence_list, \n",
    "                      incorrect_sentence_list, num_total_sentences=8633141):\n",
    "    '''\n",
    "    Ingest a list of (properly formatted) correct sentences and incorrect sentences.\n",
    "    For each list, score the negative log likelihood of each sentence.\n",
    "    Output the correct sentence in any situation where the incorrect \n",
    "    sentence scores better than the correct one.\n",
    "    output format: tuple of (correct_sentence, incorrect_sentence)\n",
    "    '''\n",
    "\n",
    "    mistakes_dict = {}\n",
    "    num_mistakes = 0\n",
    "    correct_mistakes = []\n",
    "    incorrect_mistakes = []\n",
    "    \n",
    "    for index in range(len(correct_sentence_list)):\n",
    "        correct_score = -1*(skill2vec_model.score([correct_sentence_list[index].split()], \n",
    "                                                  total_sentences=num_total_sentences))\n",
    "        incorrect_score = -1*(skill2vec_model.score([incorrect_sentence_list[index].split()], \n",
    "                                                    total_sentences=num_total_sentences))\n",
    "        \n",
    "        if incorrect_score < correct_score:\n",
    "            correct_skills = find_hard_skills(correct_sentence_list[index])\n",
    "            incorrect_skills = find_hard_skills(incorrect_sentence_list[index])\n",
    "#             pairs.append((correct_skills, incorrect_skills))\n",
    "            mistakes_dict[index] = (correct_skills, incorrect_skills)\n",
    "            num_mistakes += 1\n",
    "            correct_mistakes.append(correct_sentence_list[index])\n",
    "            incorrect_mistakes.append(incorrect_sentence_list[index])\n",
    "    return(mistakes_dict, num_mistakes, correct_mistakes, incorrect_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_hs1_neg0 468\n",
      "word2vec_hs1_neg0_cbow 361\n",
      "word2vec_hs1_neg0_size50_cbow 398\n",
      "word2vec_hs1_neg0_size200 443\n",
      "word2vec_hs1_neg0_window_8 440\n",
      "word2vec_size_400_hs1_neg0 416\n",
      "word2vec_size_600_hs1_neg0 411\n",
      "word2vec_hs1_neg0_window_10_size300 438\n",
      "word2vec_hs1_neg0_window_12_size600 425\n",
      "word2vec_hs1_neg0_window_15_size300 439\n",
      "CPU times: user 4min 15s, sys: 2min 59s, total: 7min 14s\n",
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for model in full_text_models:\n",
    "    path = 'models/' + model\n",
    "    skill2vec_model = Word2Vec.load(path)\n",
    "\n",
    "    mistakes_dict, num_mistakes, correct_mistakes, incorrect_mistakes  = analyze_sentence_fails(skill2vec_model, \n",
    "                                                                                                correct_sentences_formatted,\n",
    "                                                                                                incorrect_sentences_formatted)\n",
    "\n",
    "    \n",
    "    output_file = 'errors/dicts/' + model + '_dict.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(mistakes_dict, f)\n",
    "        \n",
    "        \n",
    "    # Write confused items to file\n",
    "    pairs_counter = defaultdict(int)\n",
    "\n",
    "    for item in mistakes_dict.values():\n",
    "        pairs_counter[str(item)] += 1\n",
    "\n",
    "    pairs_counter.default_factory = None\n",
    "    \n",
    "    outpath = 'errors/confusion/' + model + '.txt'\n",
    "    with open(outpath, 'w') as outfile:\n",
    "        s = [(k, pairs_counter[k]) for k in sorted(pairs_counter, key=pairs_counter.get, reverse=True)]\n",
    "        for k, v in s:\n",
    "\n",
    "            output = k + '\\t' + str(v) + '\\n'\n",
    "            outfile.write(output)\n",
    "\n",
    "    print(model, len(mistakes_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idx = -1\n",
    "# for k, v in mistakes_dict.items():\n",
    "    \n",
    "#     idx += 1\n",
    "#     if len(v[0]) != len(v[1]):\n",
    "#         print(idx, '*'*40)\n",
    "#         print(k, v)\n",
    "#         print(\"Correct:\",correct_mistakes[idx])\n",
    "#         print(\"Incorrect:\", incorrect_mistakes[idx])\n",
    "#         print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a confusion matrix \n",
    "\n",
    "### (or, at least, a defaultdict counter object that allows us to analyze confused pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_counter = defaultdict(int)\n",
    "\n",
    "for item in mistakes_dict.values():\n",
    "    pairs_counter[str(item)] += 1\n",
    "\n",
    "pairs_counter.default_factory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pairs_counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "\n",
    "s = [(k, pairs_counter[k]) for k in sorted(pairs_counter, key=pairs_counter.get, reverse=True)]\n",
    "\n",
    "for k, v in s[:100]:\n",
    "    output = k + '\\t' + str(v)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write most commonly mistaken words to file\n",
    "model_name = 'word2vec_hs1_neg0_window_15_size300'\n",
    "path = 'errors/' + model_name + '.txt'\n",
    "\n",
    "s = [(k, pairs_counter[k]) for k in sorted(pairs_counter, key=pairs_counter.get, reverse=True)]\n",
    "\n",
    "with open(path,'w') as outfile:\n",
    "    for k, v in s[:100]:\n",
    "        output = k + '\\t' + str(v)\n",
    "        outfile.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct counter for most often confused terms (not pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c 16\n",
      "programming_language 13\n",
      "sql 11\n",
      "stress_load 9\n",
      "gnu 9\n",
      "linux 8\n",
      "java 7\n",
      "silverlight 7\n",
      "cobol 7\n",
      "operating_system 7\n",
      "powershell 7\n",
      "statistic 7\n",
      "regression 7\n",
      "datamining 7\n",
      "dax 6\n",
      "yardis 6\n",
      "btrieve 6\n",
      "apache 6\n",
      "wxwidget 6\n",
      "citrix 6\n"
     ]
    }
   ],
   "source": [
    "skills_counter = defaultdict(int)\n",
    "\n",
    "# Read in an error dict\n",
    "\n",
    "with open('errors/dicts/baseline_skill_texts.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        skills_counter[line] += 1\n",
    "\n",
    "# Convert to regular dict\n",
    "skills_counter.default_factory = None\n",
    "\n",
    "# print(skills_counter['net'])\n",
    "\n",
    "# Sort by frequency\n",
    "s = [(k, skills_counter[k]) for k in sorted(skills_counter, key=skills_counter.get, reverse=True)]\n",
    "for k, v in s[:20]:\n",
    "    print(k, v)\n",
    "\n",
    "model_name = 'word2vec_hs1_neg0'\n",
    "path = 'errors/dicts/' + model_name + '_skills_errors_frequency.txt'\n",
    "\n",
    "with open(path,'w') as outfile:\n",
    "    for k, v in s:\n",
    "        output = k + '\\t' + str(v) + '\\n'\n",
    "        outfile.write(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine magnitude of mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 s, sys: 19.1 s, total: 46.9 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def top_magnitude_mistakes(skill2vec_model, \n",
    "                             correct_sentence_list, \n",
    "                             incorrect_sentence_list, \n",
    "                             num_total_sentences=8633141):\n",
    "    '''\n",
    "    Find the highest magnitude mistakes, \n",
    "    as magnitude seems to play more of a role than \n",
    "    quantity in the scoring scheme.\n",
    "    '''\n",
    "    correct_scores = []\n",
    "    for sentence in correct_sentence_list:\n",
    "        correct_score = -1*(skill2vec_model.score([sentence.split()], total_sentences=num_total_sentences))[0]\n",
    "        correct_scores.append(correct_score)\n",
    "    \n",
    "    incorrect_scores = []\n",
    "    for sentence in incorrect_sentence_list:\n",
    "        incorrect_score = -1*(skill2vec_model.score([sentence.split()], total_sentences=num_total_sentences))[0]\n",
    "        incorrect_scores.append(incorrect_score)\n",
    "    \n",
    "    return correct_scores, incorrect_scores\n",
    "    \n",
    "cor_top, inc_top = top_magnitude_mistakes(skill2vec_model, correct_sentences_formatted, incorrect_sentences_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "\n",
    "for i in range(len(cor_top)):\n",
    "    diffs.append(inc_top[i] - cor_top[i])\n",
    "\n",
    "top_diffs = np.argsort(diffs)\n",
    "\n",
    "for i in top_diffs[:10]:\n",
    "    print(\"Difference:\", diffs[i])\n",
    "    print(\"Correct:\", correct_sentences_formatted[i])\n",
    "    print(\"Incorrect:\", incorrect_sentences_formatted[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write most commonly mistaken words to file\n",
    "model_name = 'word2vec_hs1_neg0_window_15_size300'\n",
    "path = 'errors/' + model_name + '.txt'\n",
    "\n",
    "s = [(k, pairs_counter[k]) for k in sorted(pairs_counter, key=pairs_counter.get, reverse=True)]\n",
    "\n",
    "with open(path,'w') as outfile:\n",
    "    for k, v in s[:100]:\n",
    "        output = k + '\\t' + str(v)\n",
    "        outfile.write(output)\n",
    "\n",
    "top_diffs = np.argsort(diffs)\n",
    "\n",
    "for i in top_diffs[:10]:\n",
    "    print(\"Difference:\", diffs[i])\n",
    "    print(\"Correct:\", correct_sentences_formatted[i])\n",
    "    print(\"Incorrect:\", incorrect_sentences_formatted[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 15s, sys: 3min 7s, total: 7min 23s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for model in full_text_models:\n",
    "    path = 'models/' + model\n",
    "    skill2vec_model = Word2Vec.load(path)\n",
    "\n",
    "    cor_top, inc_top = top_magnitude_mistakes(skill2vec_model, \n",
    "                                              correct_sentences_formatted, \n",
    "                                              incorrect_sentences_formatted)\n",
    "    \n",
    "    diffs = []\n",
    "    for i in range(len(cor_top)):\n",
    "        diffs.append(inc_top[i] - cor_top[i])\n",
    "    top_diffs = np.argsort(diffs)\n",
    "\n",
    "    output_file = 'errors/big_error_sents/' + model + '.txt'\n",
    "    with open(output_file,'w') as outfile:\n",
    "        for index, i in enumerate(top_diffs[:50]):\n",
    "            diff = diffs[i]\n",
    "            cor_sent = correct_sentences_formatted[i]\n",
    "            inc_sent = incorrect_sentences_formatted[i]\n",
    "            header = \"Error \" + str(index) + '. ' + str(diff) + '\\n'\n",
    "            cor_text = \"Correct: \" + cor_sent + '\\n'\n",
    "            inc_text = \"Incorrect: \" + inc_sent + '\\n'\n",
    "            outfile.write(header)\n",
    "            outfile.write(cor_text)\n",
    "            outfile.write(inc_text)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
